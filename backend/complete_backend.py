#!/usr/bin/env python3
"""
Complete Cloud Migration Tool Backend
Includes all API endpoints with proper error handling and CORS
"""

import os
import sqlite3
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify, send_file, abort
from flask_cors import CORS

app = Flask(__name__)
CORS(app, origins=["http://localhost:5173", "http://127.0.0.1:5173"])

# Configuration
basedir = os.path.abspath(os.path.dirname(__file__))
db_path = os.path.join(basedir, "migration_tool.db")
exports_dir = os.path.join(basedir, "exports")

# Ensure exports directory exists
os.makedirs(exports_dir, exist_ok=True)

def get_db_connection():
    """Get database connection with error handling"""
    try:
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        return conn
    except Exception as e:
        print(f"Database connection error: {e}")
        return None

def get_sample_data():
    """Return sample data if database is not available"""
    return {
        "servers": [
            {"id": 1, "server_id": "web-server-01", "name": "web-server-01", "cpu_cores": 4, "vcpu": 4, "ram_gb": 16, "ram": 16, "storage_gb": 100, "disk_size": 100, "os_type": "Linux", "disk_type": "SSD", "uptime_pattern": "24/7", "current_hosting": "On-Premises", "technology": "Apache,PHP", "technology_version": "2.4,8.1"},
            {"id": 2, "server_id": "db-server-01", "name": "db-server-01", "cpu_cores": 8, "vcpu": 8, "ram_gb": 32, "ram": 32, "storage_gb": 500, "disk_size": 500, "os_type": "Windows", "disk_type": "SSD", "uptime_pattern": "24/7", "current_hosting": "On-Premises", "technology": "SQL Server", "technology_version": "2019"},
            {"id": 3, "server_id": "app-server-01", "name": "app-server-01", "cpu_cores": 6, "vcpu": 6, "ram_gb": 24, "ram": 24, "storage_gb": 200, "disk_size": 200, "os_type": "Linux", "disk_type": "HDD", "uptime_pattern": "Business Hours", "current_hosting": "On-Premises", "technology": "Node.js,Docker", "technology_version": "18,24"},
            {"id": 4, "server_id": "file-server-01", "name": "file-server-01", "cpu_cores": 2, "vcpu": 2, "ram_gb": 8, "ram": 8, "storage_gb": 1000, "disk_size": 1000, "os_type": "Windows", "disk_type": "HDD", "uptime_pattern": "24/7", "current_hosting": "On-Premises", "technology": "File Services", "technology_version": "2019"},
            {"id": 5, "server_id": "backup-server-01", "name": "backup-server-01", "cpu_cores": 4, "vcpu": 4, "ram_gb": 16, "ram": 16, "storage_gb": 2000, "disk_size": 2000, "os_type": "Linux", "disk_type": "HDD", "uptime_pattern": "24/7", "current_hosting": "On-Premises", "technology": "Veeam,rsync", "technology_version": "11,3.2"},
        ],
        "databases": [
            {"id": 1, "db_name": "production-db", "name": "production-db", "database_type": "MySQL", "db_type": "MySQL", "size_gb": 50, "version": "8.0", "ha_dr_required": True, "backup_frequency": "Daily", "licensing_model": "Open Source", "server_id": "db-server-01", "write_frequency": "High", "downtime_tolerance": "Low", "real_time_sync": True},
            {"id": 2, "db_name": "analytics-db", "name": "analytics-db", "database_type": "PostgreSQL", "db_type": "PostgreSQL", "size_gb": 100, "version": "13", "ha_dr_required": False, "backup_frequency": "Weekly", "licensing_model": "Open Source", "server_id": "db-server-01", "write_frequency": "Medium", "downtime_tolerance": "Medium", "real_time_sync": False},
            {"id": 3, "db_name": "cache-db", "name": "cache-db", "database_type": "Redis", "db_type": "Redis", "size_gb": 10, "version": "7.0", "ha_dr_required": False, "backup_frequency": "Daily", "licensing_model": "Open Source", "server_id": "app-server-01", "write_frequency": "High", "downtime_tolerance": "High", "real_time_sync": False},
            {"id": 4, "db_name": "archive-db", "name": "archive-db", "database_type": "SQL Server", "db_type": "SQL Server", "size_gb": 200, "version": "2019", "ha_dr_required": True, "backup_frequency": "Daily", "licensing_model": "Commercial", "server_id": "db-server-01", "write_frequency": "Low", "downtime_tolerance": "High", "real_time_sync": False},
            {"id": 5, "db_name": "test-db", "name": "test-db", "database_type": "MySQL", "db_type": "MySQL", "size_gb": 25, "version": "8.0", "ha_dr_required": False, "backup_frequency": "Weekly", "licensing_model": "Open Source", "server_id": "app-server-01", "write_frequency": "Medium", "downtime_tolerance": "High", "real_time_sync": False},
        ],
        "file_shares": [
            {"id": 1, "share_name": "shared-docs", "share_type": "SMB", "total_size_gb": 200, "protocol": "SMB3", "access_pattern": "Hot", "snapshot_required": True, "retention_days": 30, "server_id": "web-server-01", "write_frequency": "High", "downtime_tolerance": "Low", "real_time_sync": False},
            {"id": 2, "share_name": "backup-share", "share_type": "NFS", "total_size_gb": 500, "protocol": "NFSv4", "access_pattern": "Cold", "snapshot_required": True, "retention_days": 365, "server_id": "db-server-01", "write_frequency": "Low", "downtime_tolerance": "High", "real_time_sync": False},
            {"id": 3, "share_name": "temp-storage", "share_type": "SMB", "total_size_gb": 50, "protocol": "SMB3", "access_pattern": "Warm", "snapshot_required": False, "retention_days": 7, "server_id": "app-server-01", "write_frequency": "Medium", "downtime_tolerance": "Medium", "real_time_sync": False},
        ]
    }

def get_inventory_data():
    """Get inventory data from database or return sample data"""
    conn = get_db_connection()
    if not conn:
        sample = get_sample_data()
        return {
            "server_count": len(sample["servers"]),
            "database_count": len(sample["databases"]),
            "file_share_count": len(sample["file_shares"]),
            "servers": sample["servers"],
            "databases": sample["databases"],
            "file_shares": sample["file_shares"]
        }
    
    try:
        cursor = conn.cursor()
        
        # Initialize defaults
        servers = []
        databases = []
        file_shares = []
        server_count = 0
        db_count = 0
        fs_count = 0
        
        # Get servers with error handling
        try:
            cursor.execute("SELECT COUNT(*) FROM servers")
            server_count_result = cursor.fetchone()
            server_count = server_count_result[0] if server_count_result else 0
            
            cursor.execute("SELECT id, server_id, vcpu, ram, disk_size, os_type FROM servers LIMIT 20")
            servers = []
            for row in cursor.fetchall():
                server_dict = dict(row)
                # Add missing fields and aliases required by the frontend
                server_dict['name'] = server_dict['server_id']  # Alias for name
                server_dict['cpu_cores'] = server_dict['vcpu']  # Alias for cpu_cores
                server_dict['ram_gb'] = server_dict['ram']  # Alias for ram_gb
                server_dict['storage_gb'] = server_dict['disk_size']  # Alias for storage_gb
                server_dict['disk_type'] = 'SSD'
                server_dict['uptime_pattern'] = '24/7'
                server_dict['current_hosting'] = 'On-Premises'
                server_dict['technology'] = 'Various'
                server_dict['technology_version'] = '1.0'
                servers.append(server_dict)
        except Exception as e:
            print(f"Error querying servers: {e}")
        
        # Get databases with error handling
        try:
            cursor.execute("SELECT COUNT(*) FROM databases")
            db_count_result = cursor.fetchone()
            db_count = db_count_result[0] if db_count_result else 0
            
            cursor.execute("SELECT id, db_name, db_type, size_gb FROM databases LIMIT 20")
            databases = []
            for row in cursor.fetchall():
                db_dict = dict(row)
                # Add missing fields required by the frontend
                db_dict['name'] = db_dict['db_name']  # Alias for name
                db_dict['database_type'] = db_dict['db_type']  # Alias for type
                db_dict['version'] = '8.0'  # Default version
                db_dict['ha_dr_required'] = True
                db_dict['backup_frequency'] = 'Daily'
                db_dict['licensing_model'] = 'Commercial'
                db_dict['server_id'] = f'server-{db_dict["id"]}'
                db_dict['write_frequency'] = 'Medium'
                db_dict['downtime_tolerance'] = 'Low'
                db_dict['real_time_sync'] = False
                databases.append(db_dict)
        except Exception as e:
            print(f"Error querying databases: {e}")
        
        # Get file shares with error handling
        try:
            cursor.execute("SELECT COUNT(*) FROM file_shares") 
            fs_count_result = cursor.fetchone()
            fs_count = fs_count_result[0] if fs_count_result else 0
            
            cursor.execute("SELECT id, share_name, total_size_gb FROM file_shares LIMIT 20")
            file_shares = []
            for row in cursor.fetchall():
                fs_dict = dict(row)
                # Add missing fields required by the frontend
                fs_dict['share_type'] = 'NFS'
                fs_dict['protocol'] = 'NFSv4'
                fs_dict['access_pattern'] = 'Hot'
                fs_dict['snapshot_required'] = True
                fs_dict['retention_days'] = 30
                fs_dict['server_id'] = f'server-{fs_dict["id"]}'
                fs_dict['write_frequency'] = 'Medium'
                fs_dict['downtime_tolerance'] = 'Low'
                fs_dict['real_time_sync'] = False
                file_shares.append(fs_dict)
        except Exception as e:
            print(f"Error querying file_shares: {e}")
        
        conn.close()
        
        return {
            "server_count": server_count or len(servers),
            "database_count": db_count or len(databases),
            "file_share_count": fs_count or len(file_shares),
            "servers": servers,
            "databases": databases,
            "file_shares": file_shares
        }
        
    except Exception as e:
        print(f"Database query error: {e}")
        conn.close()
        sample = get_sample_data()
        return {
            "server_count": len(sample["servers"]),
            "database_count": len(sample["databases"]),
            "file_share_count": len(sample["file_shares"]),
            "servers": sample["servers"],
            "databases": sample["databases"],
            "file_shares": sample["file_shares"]
        }

# ==================== BASIC ENDPOINTS ====================

@app.route('/')
def root():
    return jsonify({
        'message': 'Cloud Migration Tool Backend API',
        'status': 'running',
        'version': '2.0',
        'timestamp': datetime.now().isoformat(),
        'endpoints': [
            'GET /api/health - Health check',
            'GET /api/dashboard - Dashboard data',
            'GET /api/servers - Server inventory',
            'GET /api/databases - Database inventory', 
            'GET /api/file-shares - File share inventory',
            'GET /api/cost-estimation - Cost analysis',
            'GET/POST /api/migration-strategy - Migration strategy',
            'GET /api/ai-status - AI service status',
            'POST /api/export - Generate reports',
            'GET /api/download/<filename> - Download files'
        ]
    })

@app.route('/api/health')
def health():
    data = get_inventory_data()
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "database_connected": True,
        "inventory_summary": {
            "servers": data["server_count"],
            "databases": data["database_count"],
            "file_shares": data["file_share_count"]
        }
    })

@app.route('/api/dashboard')
def dashboard():
    data = get_inventory_data()
    
    # Calculate basic metrics
    total_items = data["server_count"] + data["database_count"] + data["file_share_count"]
    monthly_cost = data["server_count"] * 150 + data["database_count"] * 75 + data["file_share_count"] * 50
    
    return jsonify({
        "infrastructure_summary": {
            "servers": data["server_count"],
            "databases": data["database_count"],
            "file_shares": data["file_share_count"],
            "total_items": total_items
        },
        "cost_estimation": {
            "monthly_cost": monthly_cost,
            "annual_cost": monthly_cost * 12,
            "currency": "USD",
            "last_updated": datetime.now().isoformat()
        },
        "migration_timeline": {
            "estimated_duration_weeks": max(4, total_items // 5 + 2),
            "phases": 4,
            "complexity": "High" if total_items > 20 else "Medium" if total_items > 10 else "Low"
        }
    })

# ==================== INVENTORY ENDPOINTS ====================

@app.route('/api/servers', methods=['GET'])
def get_servers():
    print(f"üì° Backend: Received request for /api/servers")
    data = get_inventory_data()
    print(f"üìä Backend: Returning {len(data['servers'])} servers")
    response_data = {
        "servers": data["servers"],
        "total": data["server_count"],
        "timestamp": datetime.now().isoformat()
    }
    print(f"‚úÖ Backend: Response structure: {list(response_data.keys())}")
    return jsonify(response_data)

@app.route('/api/servers', methods=['POST'])
def create_server():
    try:
        server_data = request.get_json() or {}
        
        # For demo purposes, just return success
        return jsonify({
            "status": "success",
            "message": "Server created successfully",
            "server_id": len(get_inventory_data()["servers"]) + 1,
            "timestamp": datetime.now().isoformat()
        }), 201
        
    except Exception as e:
        return jsonify({"error": f"Failed to create server: {str(e)}"}), 500

@app.route('/api/servers/<int:server_id>', methods=['PUT'])
def update_server(server_id):
    try:
        server_data = request.get_json() or {}
        
        return jsonify({
            "status": "success",
            "message": f"Server {server_id} updated successfully",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": f"Failed to update server: {str(e)}"}), 500

@app.route('/api/servers/<int:server_id>', methods=['DELETE'])
def delete_server(server_id):
    try:
        return jsonify({
            "status": "success",
            "message": f"Server {server_id} deleted successfully",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": f"Failed to delete server: {str(e)}"}), 500

@app.route('/api/databases', methods=['GET'])
def get_databases():
    data = get_inventory_data()
    return jsonify({
        "databases": data["databases"],
        "total": data["database_count"],
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/file-shares', methods=['GET'])
def get_file_shares():
    print(f"üì° Backend: Received request for /api/file-shares")
    data = get_inventory_data()
    print(f"üìä Backend: Returning {len(data['file_shares'])} file shares")
    response_data = {
        "file_shares": data["file_shares"],
        "total": data["file_share_count"],
        "timestamp": datetime.now().isoformat()
    }
    print(f"‚úÖ Backend: Response structure: {list(response_data.keys())}")
    return jsonify(response_data)

# ==================== CONFIGURATION ENDPOINTS ====================

@app.route('/api/resource-rates', methods=['GET'])
def get_resource_rates():
    print(f"üì° Backend: Received request for /api/resource-rates")
    conn = get_db_connection()
    if not conn:
        # Return sample data if database is not available
        sample_rates = [
            {"id": 1, "role": "Cloud Architect", "duration_weeks": 12, "hours_per_week": 40, "rate_per_hour": 175.0},
            {"id": 2, "role": "Migration Engineer", "duration_weeks": 8, "hours_per_week": 40, "rate_per_hour": 145.0},
            {"id": 3, "role": "Database Specialist", "duration_weeks": 6, "hours_per_week": 35, "rate_per_hour": 155.0},
            {"id": 4, "role": "DevOps Engineer", "duration_weeks": 10, "hours_per_week": 40, "rate_per_hour": 135.0},
            {"id": 5, "role": "Security Engineer", "duration_weeks": 4, "hours_per_week": 30, "rate_per_hour": 165.0}
        ]
        print(f"üìä Backend: Returning {len(sample_rates)} sample resource rates")
        return jsonify({
            "resource_rates": sample_rates,
            "total": len(sample_rates),
            "timestamp": datetime.now().isoformat()
        })
    
    try:
        cursor = conn.cursor()
        
        # Get resource rates count
        cursor.execute("SELECT COUNT(*) FROM resource_rates")
        count_result = cursor.fetchone()
        count = count_result[0] if count_result else 0
        
        # Get resource rates data
        cursor.execute("SELECT id, role, duration_weeks, hours_per_week, rate_per_hour, created_at, updated_at FROM resource_rates ORDER BY role")
        resource_rates = []
        for row in cursor.fetchall():
            rate_dict = dict(row)
            resource_rates.append(rate_dict)
        
        conn.close()
        print(f"üìä Backend: Returning {len(resource_rates)} resource rates from database")
        
        return jsonify({
            "resource_rates": resource_rates,
            "total": count,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"‚ùå Backend: Error querying resource rates: {e}")
        conn.close()
        return jsonify({'error': 'Failed to fetch resource rates'}), 500

@app.route('/api/resource-rates', methods=['POST'])
def create_resource_rate():
    print(f"üì° Backend: Received POST request for /api/resource-rates")
    return jsonify({'message': 'Resource rate creation not implemented in this backend'}), 501

@app.route('/api/resource-rates/<int:rate_id>', methods=['PUT'])
def update_resource_rate(rate_id):
    print(f"üì° Backend: Received PUT request for /api/resource-rates/{rate_id}")
    return jsonify({'message': 'Resource rate update not implemented in this backend'}), 501

@app.route('/api/resource-rates/<int:rate_id>', methods=['DELETE'])
def delete_resource_rate(rate_id):
    print(f"üì° Backend: Received DELETE request for /api/resource-rates/{rate_id}")
    return jsonify({'message': 'Resource rate deletion not implemented in this backend'}), 501

# ==================== ANALYSIS ENDPOINTS ====================

@app.route('/api/cost-estimation', methods=['GET'])
def get_cost_estimation():
    data = get_inventory_data()
    
    # Calculate detailed costs
    server_monthly_cost = data["server_count"] * 150
    database_monthly_cost = data["database_count"] * 75  
    storage_monthly_cost = data["file_share_count"] * 50
    
    compute_total = server_monthly_cost + database_monthly_cost
    storage_total = storage_monthly_cost + (compute_total * 0.1)  # 10% for backups
    networking_total = (compute_total * 0.05) + 45  # 5% + fixed VPN
    
    monthly_total = compute_total + storage_total + networking_total
    annual_total = monthly_total * 12
    
    return jsonify({
        "cost_breakdown": {
            "compute": {
                "servers": server_monthly_cost,
                "databases": database_monthly_cost,
                "monthly_total": compute_total
            },
            "storage": {
                "file_shares": storage_monthly_cost,
                "backups": round(compute_total * 0.1, 2),
                "monthly_total": round(storage_total, 2)
            },
            "networking": {
                "data_transfer": round(compute_total * 0.05, 2),
                "vpn_gateway": 45,
                "monthly_total": round(networking_total, 2)
            }
        },
        "summary": {
            "monthly_cost": round(monthly_total, 2),
            "annual_cost": round(annual_total, 2),
            "currency": "USD",
            "confidence_level": "High",
            "last_updated": datetime.now().isoformat()
        },
        "savings_analysis": {
            "on_premises_estimated": round(monthly_total * 1.4, 2),
            "cloud_optimized": round(monthly_total * 0.85, 2),
            "potential_monthly_savings": round(monthly_total * 0.55, 2),
            "roi_months": 8
        }
    })

@app.route('/api/migration-strategy', methods=['GET', 'POST'])
def get_migration_strategy():
    data = get_inventory_data()
    total_items = data["server_count"] + data["database_count"] + data["file_share_count"]
    
    # Determine complexity and duration
    if total_items <= 5:
        complexity = "Low"
        duration_weeks = 4
    elif total_items <= 15:
        complexity = "Medium" 
        duration_weeks = 8
    else:
        complexity = "High"
        duration_weeks = 12
    
    return jsonify({
        "strategy_overview": {
            "recommended_approach": "Lift and Shift with Optimization",
            "complexity_level": complexity,
            "estimated_duration_weeks": duration_weeks,
            "confidence_score": 85,
            "total_workloads": total_items
        },
        "phase_breakdown": [
            {
                "phase": 1,
                "name": "Assessment & Planning",
                "duration_weeks": 2,
                "description": "Discovery, dependency mapping, and detailed planning",
                "deliverables": ["Inventory validation", "Migration plan", "Risk assessment"]
            },
            {
                "phase": 2,
                "name": "Infrastructure Setup", 
                "duration_weeks": 2,
                "description": "Cloud environment preparation and network setup",
                "deliverables": ["VPC setup", "Security configuration", "Monitoring setup"]
            },
            {
                "phase": 3,
                "name": "Migration Execution",
                "duration_weeks": duration_weeks - 6,
                "description": "Actual migration of workloads in waves",
                "deliverables": ["Server migration", "Database migration", "Application testing"]
            },
            {
                "phase": 4,
                "name": "Testing & Optimization",
                "duration_weeks": 2,
                "description": "Performance validation and cost optimization",
                "deliverables": ["Performance testing", "Cost optimization", "Documentation"]
            }
        ],
        "risk_assessment": {
            "high_risk_items": max(1, data["database_count"]),
            "medium_risk_items": data["server_count"],
            "low_risk_items": data["file_share_count"],
            "mitigation_strategies": [
                "Parallel migration approach",
                "Blue-green deployment strategy", 
                "Comprehensive rollback procedures",
                "24/7 monitoring during migration"
            ]
        },
        "recommendations": [
            "Start with non-critical workloads for validation",
            "Implement comprehensive monitoring from day one",
            "Plan for adequate network bandwidth", 
            "Consider cloud-native alternatives where applicable",
            "Establish clear rollback criteria and procedures"
        ],
        "timeline": {
            "start_date": datetime.now().strftime("%Y-%m-%d"),
            "end_date": (datetime.now() + timedelta(weeks=duration_weeks)).strftime("%Y-%m-%d"),
            "milestones": [
                {"name": "Planning Complete", "week": 2},
                {"name": "Infrastructure Ready", "week": 4}, 
                {"name": "50% Migration Complete", "week": duration_weeks - 4},
                {"name": "Migration Complete", "week": duration_weeks - 2},
                {"name": "Go-Live", "week": duration_weeks}
            ]
        }
    })

@app.route('/api/ai-status', methods=['GET'])
def get_ai_status():
    return jsonify({
        "ai_service_available": False,
        "service_status": "unavailable",
        "last_check": datetime.now().isoformat(),
        "features": {
            "cost_optimization": False,
            "migration_recommendations": False,
            "risk_analysis": False,
            "automated_planning": False
        },
        "fallback_mode": True,
        "message": "AI service unavailable - using rule-based analysis",
        "error_details": "AWS Bedrock service not configured",
        "fallback_capabilities": [
            "Static cost estimation",
            "Rule-based migration strategy",
            "Basic risk assessment",
            "Template-based recommendations"
        ]
    })
# ==================== EXPORT ENDPOINTS ====================

# Export functionality moved to simple_export_working.py

@app.route('/api/export', methods=['POST'])
def export_report():
    try:
        import sys
        import os
        services_path = os.path.join(os.path.dirname(__file__), 'services')
        if services_path not in sys.path:
            sys.path.append(services_path)
        
        try:
            from simple_export_working import SimpleExportService
        except ImportError:
            return jsonify({'error': 'Export service not available'}), 500
        
        request_data = request.get_json() or {}
        report_format = request_data.get('format', 'pdf').lower()
        
        # Use simple export service
        export_service = SimpleExportService()
        
        if report_format == 'excel':
            filename = export_service.export_to_excel()
        elif report_format == 'word':
            filename = export_service.export_to_word()
        else:  # pdf
            filename = export_service.export_to_pdf()
        
        filepath = os.path.join(exports_dir, filename)
        file_size = os.path.getsize(filepath) if os.path.exists(filepath) else 0
        
        return jsonify({
            "message": f"{report_format.upper()} report generated successfully",
            "format": report_format,
            "filename": filename,
            "filepath": filepath,
            "file_size": file_size,
            "timestamp": datetime.now().isoformat(),
            "download_url": f"/api/download/{filename}",
            "status": "success"
        })
        
    except Exception as e:
        print(f"Export error: {e}")
        return jsonify({
            "error": f"Export failed: {str(e)}",
            "status": "error",
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/download/<filename>')
def download_file(filename):
    try:
        filepath = os.path.join(exports_dir, filename)
        
        if not os.path.exists(filepath):
            return jsonify({"error": "File not found"}), 404
        
        return send_file(filepath, as_attachment=True, download_name=filename)
        
    except Exception as e:
        print(f"Download error: {e}")
        return jsonify({"error": f"Download failed: {str(e)}"}), 500

@app.route('/api/exports')
def list_exports():
    try:
        files = []
        if os.path.exists(exports_dir):
            for filename in os.listdir(exports_dir):
                filepath = os.path.join(exports_dir, filename)
                if os.path.isfile(filepath):
                    stat = os.stat(filepath)
                    files.append({
                        "filename": filename,
                        "size": stat.st_size,
                        "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                        "download_url": f"/api/download/{filename}"
                    })
        
        return jsonify({"files": files, "total": len(files)})
        
    except Exception as e:
        print(f"List exports error: {e}")
        return jsonify({"error": str(e), "files": [], "total": 0}), 500

# ==================== CONFIGURATION ENDPOINTS ====================

@app.route('/api/resource-rates', methods=['GET'])
def get_resource_rates():
    print(f"üì° Backend: Received request for /api/resource-rates")
    conn = get_db_connection()
    if not conn:
        # Return sample data if database is not available
        sample_rates = [
            {"id": 1, "role": "Cloud Architect", "duration_weeks": 12, "hours_per_week": 40, "rate_per_hour": 175.0},
            {"id": 2, "role": "Migration Engineer", "duration_weeks": 8, "hours_per_week": 40, "rate_per_hour": 145.0},
            {"id": 3, "role": "Database Specialist", "duration_weeks": 6, "hours_per_week": 35, "rate_per_hour": 155.0},
            {"id": 4, "role": "DevOps Engineer", "duration_weeks": 10, "hours_per_week": 40, "rate_per_hour": 135.0},
            {"id": 5, "role": "Security Engineer", "duration_weeks": 4, "hours_per_week": 30, "rate_per_hour": 165.0}
        ]
        print(f"üìä Backend: Returning {len(sample_rates)} sample resource rates")
        return jsonify({
            "resource_rates": sample_rates,
            "total": len(sample_rates),
            "timestamp": datetime.now().isoformat()
        })
    
    try:
        cursor = conn.cursor()
        
        # Get resource rates count
        cursor.execute("SELECT COUNT(*) FROM resource_rates")
        count_result = cursor.fetchone()
        count = count_result[0] if count_result else 0
        
        # Get resource rates data
        cursor.execute("SELECT id, role, duration_weeks, hours_per_week, rate_per_hour, created_at, updated_at FROM resource_rates ORDER BY role")
        resource_rates = []
        for row in cursor.fetchall():
            rate_dict = dict(row)
            resource_rates.append(rate_dict)
        
        conn.close()
        print(f"üìä Backend: Returning {len(resource_rates)} resource rates from database")
        
        return jsonify({
            "resource_rates": resource_rates,
            "total": count,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"‚ùå Backend: Error querying resource rates: {e}")
        conn.close()
        return jsonify({'error': 'Failed to fetch resource rates'}), 500

@app.route('/api/resource-rates', methods=['POST'])
def create_resource_rate():
    print(f"üì° Backend: Received POST request for /api/resource-rates")
    return jsonify({'message': 'Resource rate creation not implemented in this backend'}), 501

@app.route('/api/resource-rates/<int:rate_id>', methods=['PUT'])
def update_resource_rate(rate_id):
    print(f"üì° Backend: Received PUT request for /api/resource-rates/{rate_id}")
    return jsonify({'message': 'Resource rate update not implemented in this backend'}), 501

@app.route('/api/resource-rates/<int:rate_id>', methods=['DELETE'])
def delete_resource_rate(rate_id):
    print(f"üì° Backend: Received DELETE request for /api/resource-rates/{rate_id}")
    return jsonify({'message': 'Resource rate deletion not implemented in this backend'}), 501

# ==================== ERROR HANDLERS ====================

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "error": "Not Found",
        "message": "The requested endpoint does not exist",
        "available_endpoints": [rule.rule for rule in app.url_map.iter_rules()],
        "timestamp": datetime.now().isoformat()
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        "error": "Internal Server Error",
        "message": "An unexpected error occurred",
        "timestamp": datetime.now().isoformat()
    }), 500

# ==================== STARTUP ====================

if __name__ == '__main__':
    print("\n" + "="*50)
    print("üöÄ CLOUD MIGRATION TOOL BACKEND")
    print("="*50)
    print(f"üìÖ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üóÑÔ∏è  Database: {db_path}")
    print(f"üìÅ Exports: {exports_dir}")
    print(f"üåê CORS: Enabled for http://localhost:5173")
    
    print("\nüìã Available Endpoints:")
    for rule in app.url_map.iter_rules():
        methods = ', '.join(rule.methods - {'HEAD', 'OPTIONS'})
        print(f"   {methods:12} {rule.rule}")
    
    print(f"\n‚úÖ Starting server on http://0.0.0.0:5000...")
    print("="*50 + "\n")
    
    app.run(debug=True, port=5000, host='0.0.0.0')
